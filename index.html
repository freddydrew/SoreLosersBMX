<html lang="en">
  <head>
    <!--Standard HTML setup contained in the meta tags of the head element-->
    <meta charset="UTF-8"> 
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="styles.css"/>
    <title>
      Sore Losers BMX Home
    </title>
  </head>


  <body>
    <header class="main-header">
      <nav class="main-nav"> <!--Navigation or menu element on this page, usually on top or side-->
            <a href="index.html">HOME</a>   
            <a href="about.html">ABOUT</a>
      </nav> 
      <h1>SORE LOSERS BMX</h1>
    </header>

    <section>
      <h2></h2>
      <p>
        Section 2: Arrays and Big-O Notation

Time complexity: (In order of speed, fastest to slowest)
Constant		O(1)
Logarithmic		O(LogN)***
Linear 			O(N)
N Log-Star N 		O(N LogN)***
Quadratic 		O(N^2)
***These are Log base 2, NOT 10

Big-O gives a way of comparing time of algorithms in a hardware independent manner.

Arrays in Memory
Arrays are stored contiguously in memory, not like linked lists that can hop around memory. We denote array size at allocation so the computer knows how much space it needs to free up in a row. 
Each array element is the same size in terms of number of bits.
An array of objects holds the references to each instance, like a string array or random type of Object that you created. 
Because the array is contiguous and the elements are the same size, we can easily index array elements, and get something at a given index instantly.
The formula is element[x] = address of element 0 + (x*size of each element in bits) 
Section 3: Sort Algorithms
Bubble Sort 
Selection Sort
Insertion Sort
Shell Sort
Merge Sort
Quick Sort
Counting Sort
Radix Sort

Bubble Sort
It splits the array into a sorted and unsorted partition
Swaps adjacent elements and goes through the array as many times as it takes to get them all in order, 
For loop nested in another, first decrements the end of the sorted partition, second for loop increments the elements and stops at the end of the sorted partition
O(N^2) (an approximation kinda)
In-place algorithm 
Degrades pretty quickly
Stable sort algorithm

Stable Vs Unstable Sort Algorithms
Comes into play when thereâ€™s duplicate values in an array- What happens when theyâ€™re next to each other?
Unstable sort - they are not in the same relative order WRT each other
Stable sort - the relative positions WRT each other is preserved 
If sorting objects it gets complicated, integers not so much

Selection Sort
Traverse array and find largest/smallest and swap it with the last element, then traverse the unsorted partition again and repeat
O(N^2) quadratic
In-place algorithm 
Only swaps once a traversal
Unstable sort algorithm (swaps largest element in unsorted partition so it doesnâ€™t account for duplicates)

Insertion Sort
Partition the array into sorted, unsorted
Grows the sorted partition from the front of the array
Compare value to be sorted to those in the sorted array, moving right to left
O(N^2) quadratic
In-place algorithm 
Stable sort algorithm

Shell Sort
Variation of insertion sort
Insertion sort does a gap of 1 to slide things over, shell sort starts with a larger gap and shrinks it as it goes, reducing the number of needed shifts
When the gap value hits 1, itâ€™s an insertion sort. This will always happen at the end of the sorting process
Instead of comparing values to a neighbor, it compares them to a group of values within the gap size
By the time we get to an insertion sort stage, most of the array is sorted
A common way to find initial gap value is using the knuth sequence
Shell Sort Algorithm Example
Time complexity depends on the gap calculation 
In-place algorithm 
Unstable sort algorithm

Merge Sort
Divide and conquer array (splitting array into smaller ones)
Usually written recursively 
First phase is splitting, then merging phase which is where sorting is done
Splitting leads to faster sorting, is only logical and is not actually done using new array objects
Merge sort ðŸ”ª
O (N log N) Logarithmic,  Quasilinear time
Not in-place, space could be an issue
Stable Algorithm
Usually written recursively with a helper method

Quick Sort
Divide and conquer
Recursive
Uses a pivot element to split the array
Less than the pivot goes to the left, greater than to the right of the pivot element
At the end, the pivot is in the right place but everything around it is not sorted with respect to each other. Then we repeat the process.
Quick sort âš¡
In-place
O(NLogN) Logarithmic, Quasilinear time
Unstable

Counting Sort
No comparisons
Keeps a count of distinct values
Only works with non-negative discrete values of integers
Values must be within a specific range
You count how many of each digit there is, and then you determine each digit's starting position by counting how many cells are taken up by the digits before it
Counting Sort Explained and Implemented with Examples in Java | Sorting Algorithms | Geekific
Not in-place
O(N)
Could be stable

Radix Sort
Radix is the number of unique values in a numbering system of alphabet
Data has to have the same radix and width
Same logic as counting sort, but accounts for bigger range of values
LSD vs MSD Radix sort just starts on different significant digits of the given value
Do a counting sort on each digit in the value
Canâ€™t be used on floats
Not in-place
O(N)
Stable


      </p>

    </section>

    <footer class="footer">
      <h3>Follow us on spotify and instagram!</h3>
      <nav class="footer-nav">
          <a href="https://open.spotify.com/show/2o9sKDNPgEBzczCOUoB151?si=ba9ebfa20e1644f5">
            <img src="/images/SpotifyLogo.png" width="25" height="25">
          </a>
          <a href="https://www.instagram.com/sorelosersbmxpodcast/?hl=en">
            <img src="/images/InstragramLogo.png" width="25" height="25">
          </a>
      </nav>
    </footer>
  </body>

</html>